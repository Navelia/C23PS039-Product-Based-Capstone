{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+xEoVOXvVlxMyCye70JYO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Navelia/C23PS039-Product-Based-Capstone/blob/master/Bisindo_Recognition_model_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ISLRvHiDhDam"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import RMSprop"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explore data"
      ],
      "metadata": {
        "id": "TeAf1QZGiG7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Who6kUg8kHC7",
        "outputId": "715c74d5-e311-4f4b-fab2-3841557541d4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/MyDrive/Dataset ReachYou/Bisindo Huruf/\""
      ],
      "metadata": {
        "id": "rKEabHO9kXWI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20852125-69da-486e-a649-033e1022d206"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bisindo_1.zip  bisindo_2.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bisindo_1_zip = \"/content/drive/MyDrive/Dataset ReachYou/Bisindo Huruf/bisindo_1.zip\"\n",
        "bisindo_2_zip = \"/content/drive/MyDrive/Dataset ReachYou/Bisindo Huruf/bisindo_2.zip\"\n",
        "\n",
        "zip_1 = zipfile.ZipFile(bisindo_1_zip, 'r')\n",
        "zip_2 = zipfile.ZipFile(bisindo_2_zip, 'r')\n",
        "\n",
        "zip_1.extractall(\"./data\")\n",
        "zip_1.close()"
      ],
      "metadata": {
        "id": "mbzS3GQ1lmV_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = './data/Dataset BISINDO/'\n",
        "\n",
        "train_dir = os.path.join(base_dir, 'datatrain')\n",
        "test_dir = os.path.join(base_dir, 'datatest')"
      ],
      "metadata": {
        "id": "M0DnMJxqpeeB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing"
      ],
      "metadata": {
        "id": "_YfIlb_ypxmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255.,\n",
        "      rotation_range=40,\n",
        "      width_shift_range=.2,\n",
        "      height_shift_range=.2,\n",
        "      shear_range=.2,\n",
        "      zoom_range=.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest'\n",
        "  )\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "    rescale=1./255.\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(train_dir,\n",
        "                                                    batch_size=16,\n",
        "                                                    target_size=(28,28))\n",
        "\n",
        "test_generator = train_datagen.flow_from_directory(test_dir,\n",
        "                                                    batch_size=16,\n",
        "                                                    target_size=(28,28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NDh2C0ao48S",
        "outputId": "7d3a8a68-0418-4839-e623-126c32cad5eb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1727 images belonging to 27 classes.\n",
            "Found 432 images belonging to 27 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating Model"
      ],
      "metadata": {
        "id": "wyzFxkkM8T6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "def create_model():\n",
        "\n",
        "  model = tf.keras.models.Sequential([\n",
        "      tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(28,28,3)),\n",
        "      tf.keras.layers.MaxPooling2D((2,2)),\n",
        "\n",
        "      tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "      tf.keras.layers.MaxPooling2D((2,2)),\n",
        "\n",
        "      tf.keras.layers.Flatten(),\n",
        "\n",
        "      tf.keras.layers.Dense(1024, activation='relu'),\n",
        "      tf.keras.layers.Dense(27, activation='softmax'),\n",
        "  ])\n",
        "\n",
        "\n",
        "  model.compile(optimizer = RMSprop(learning_rate=0.001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "  \n",
        "  return model"
      ],
      "metadata": {
        "id": "NW1IzliP8SwY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model()"
      ],
      "metadata": {
        "id": "QYj7rMsE-GF_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUi_vJHe82tj",
        "outputId": "fdbdced2-7d3b-4bc2-e5ea-377299e8dcd7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 26, 26, 16)        448       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 13, 13, 16)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 11, 11, 32)        4640      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 5, 5, 32)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 800)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1024)              820224    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 27)                27675     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 852,987\n",
            "Trainable params: 852,987\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "OjEntwq39O9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_generator,\n",
        "                    validation_data=test_generator,\n",
        "                    epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcnbLHOf9ODP",
        "outputId": "5714e4cb-40d6-41b1-d576-32f780b8d989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "108/108 [==============================] - 165s 2s/step - loss: 3.1361 - accuracy: 0.0845 - val_loss: 2.8547 - val_accuracy: 0.1644\n",
            "Epoch 2/100\n",
            "108/108 [==============================] - 165s 2s/step - loss: 2.6077 - accuracy: 0.2206 - val_loss: 2.4545 - val_accuracy: 0.2639\n",
            "Epoch 3/100\n",
            "108/108 [==============================] - 158s 1s/step - loss: 2.3501 - accuracy: 0.2774 - val_loss: 2.8892 - val_accuracy: 0.1644\n",
            "Epoch 4/100\n",
            "108/108 [==============================] - 163s 2s/step - loss: 2.1582 - accuracy: 0.3144 - val_loss: 2.1059 - val_accuracy: 0.3241\n",
            "Epoch 5/100\n",
            "108/108 [==============================] - 164s 2s/step - loss: 2.0002 - accuracy: 0.3642 - val_loss: 2.2075 - val_accuracy: 0.3171\n",
            "Epoch 6/100\n",
            "108/108 [==============================] - 155s 1s/step - loss: 1.8815 - accuracy: 0.3972 - val_loss: 1.9826 - val_accuracy: 0.3727\n",
            "Epoch 7/100\n",
            "108/108 [==============================] - 165s 2s/step - loss: 1.7772 - accuracy: 0.4360 - val_loss: 1.9672 - val_accuracy: 0.3495\n",
            "Epoch 8/100\n",
            "108/108 [==============================] - 153s 1s/step - loss: 1.6625 - accuracy: 0.4673 - val_loss: 1.6954 - val_accuracy: 0.4676\n",
            "Epoch 9/100\n",
            "108/108 [==============================] - 153s 1s/step - loss: 1.5707 - accuracy: 0.4800 - val_loss: 2.8878 - val_accuracy: 0.1829\n",
            "Epoch 10/100\n",
            "108/108 [==============================] - 161s 1s/step - loss: 1.4694 - accuracy: 0.5072 - val_loss: 1.6220 - val_accuracy: 0.4977\n",
            "Epoch 11/100\n",
            "108/108 [==============================] - 162s 2s/step - loss: 1.4433 - accuracy: 0.5327 - val_loss: 1.5790 - val_accuracy: 0.5046\n",
            "Epoch 12/100\n",
            "108/108 [==============================] - 149s 1s/step - loss: 1.3142 - accuracy: 0.5750 - val_loss: 1.5281 - val_accuracy: 0.5347\n",
            "Epoch 13/100\n",
            "108/108 [==============================] - 160s 1s/step - loss: 1.2940 - accuracy: 0.5860 - val_loss: 1.7216 - val_accuracy: 0.4676\n",
            "Epoch 14/100\n",
            "108/108 [==============================] - 149s 1s/step - loss: 1.2233 - accuracy: 0.6202 - val_loss: 1.3774 - val_accuracy: 0.5417\n",
            "Epoch 15/100\n",
            "108/108 [==============================] - 148s 1s/step - loss: 1.1141 - accuracy: 0.6393 - val_loss: 1.4743 - val_accuracy: 0.4977\n",
            "Epoch 16/100\n",
            "108/108 [==============================] - 160s 1s/step - loss: 1.0573 - accuracy: 0.6445 - val_loss: 1.5611 - val_accuracy: 0.5463\n",
            "Epoch 17/100\n",
            "108/108 [==============================] - 148s 1s/step - loss: 1.0246 - accuracy: 0.6757 - val_loss: 1.2687 - val_accuracy: 0.5648\n",
            "Epoch 18/100\n",
            "108/108 [==============================] - 148s 1s/step - loss: 0.9455 - accuracy: 0.6948 - val_loss: 1.3732 - val_accuracy: 0.5741\n",
            "Epoch 19/100\n",
            "108/108 [==============================] - 159s 1s/step - loss: 0.8737 - accuracy: 0.7070 - val_loss: 1.1927 - val_accuracy: 0.6273\n",
            "Epoch 20/100\n",
            "108/108 [==============================] - 160s 1s/step - loss: 0.8444 - accuracy: 0.7192 - val_loss: 1.1308 - val_accuracy: 0.6597\n",
            "Epoch 21/100\n",
            "108/108 [==============================] - 161s 1s/step - loss: 0.8142 - accuracy: 0.7313 - val_loss: 1.0098 - val_accuracy: 0.6782\n",
            "Epoch 22/100\n",
            "108/108 [==============================] - 160s 1s/step - loss: 0.7952 - accuracy: 0.7470 - val_loss: 1.3705 - val_accuracy: 0.5972\n",
            "Epoch 23/100\n",
            "108/108 [==============================] - 160s 1s/step - loss: 0.7675 - accuracy: 0.7516 - val_loss: 1.2943 - val_accuracy: 0.6157\n",
            "Epoch 24/100\n",
            "108/108 [==============================] - 199s 2s/step - loss: 0.7195 - accuracy: 0.7701 - val_loss: 0.8375 - val_accuracy: 0.7199\n",
            "Epoch 25/100\n",
            "108/108 [==============================] - 166s 2s/step - loss: 0.6885 - accuracy: 0.7695 - val_loss: 1.3666 - val_accuracy: 0.5856\n",
            "Epoch 26/100\n",
            " 24/108 [=====>........................] - ETA: 1:35 - loss: 0.7184 - accuracy: 0.7598"
          ]
        }
      ]
    }
  ]
}